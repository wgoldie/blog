{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Automatic Differentiation, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent results in deep and convolutional neural networks with powerful industrial applications have led to widespread adoption of these techniques outside of academia. One key advance has been the introduction of automatic differentiation, which allows developers and scientists to write code using standard numerical routines and functions that can be differentiated and used in training neural networks or other machine learning models. Many developers working with neural networks today, however, do not have a strong understanding of how automatic differentiation functions \"under the hood,\" as well as how it differs from symbolic and numerical differentiation. In this article, using the style of literate programming, I will build some toy domain-specific languages that model how automatic differentiation functions in popular libraries like Tensorflow and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this is not a primer on automatic differentiation itself or the underlying theory. There are many high-quality sources on these topics, which I will not hope to outdo. The purpose of this article is to show, to those familiar with the concepts of AD but not the implementation, how simply automatic differentiation can be implemented. \n",
    "\n",
    "For this reason I will present minimal background on AD and the alternatives and invite readers to search out these topics themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly three ways to perform mathematical differentiation that most people are aware of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the application of rules on abstract symbols to analytically calculate formulas for the derivative of $f$ at $x$ (where $x$ may be high dimensional). \n",
    "\n",
    "Symbolic differentiation involves a *computer algebra system* (CAS). Some people differentiate between symbolic differentiation and \"manual\" differentiation, where the latter is the same essential procedure as the former, but is performed, generally on paper, by a human.\n",
    "\n",
    "In practice, this would be something like a CAS looking up a rule in a table to convert an expression $y = x^2$, consisting of exponentiation of a scalar value, to a derivative expression $\\frac{dy}{dx} = 2x$, without ever considering the value of x itself. Often, this would occur without even considering the value of *2*. Frege, at that point, is rolling in his grave. Alternatively, this would consist of a very bored 16-year-old sitting in math class looking up this rule in a table to generate the second expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the computation of an approximate derivative of $f$ at $x$ using numerical algorithms. \n",
    "\n",
    "The most common techniques are forms of finite difference estimation, which consists of perturbing the value of $x$ and analyzing the resultant change in $f(x)$. Again, this can be done by massive linear algebra algorithms, or bored 16-year-olds.\n",
    "\n",
    "While this technique seems hacky, it's commonly used in engineering and science. It's important to keep in mind that we are working with floating point numbers of finite precision in most cases, and sometimes a \"hacky\" method can max out our precision. At the same time, numerical differentiation requires significant attention to floating point arithmetic and error explosion.\n",
    "\n",
    "Here, the derivative is computed from concrete values, and we do not obtain any structured expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "This is the computation of an exact (to machine arithmetic precision) derivative of $f$ at $x$ by analyzing the *computation* of $f(x)$. Rather than analyzing symbols that express mathematical operations (as in symbolic differentiation), or perturbing computations that represent mathematical operations, we analyze a program that computes a function $f$. This analysis is far simpler than that of symbolic algebra expressions, and can be formalized into predictable algorithms and procedures.\n",
    "\n",
    "The concept behind automatic differentiation is simple: every operation that our program is allowed to perform is shadowed by an operation that can compute the derivative of that operation. In practice, we record operations on values, and then use the \"shadow\" operations to compute derivatives of these operations using the chain rule. You can imagine all of the functions in your programming environment being doubled in this way. With the right abstraction layer, a normally written program can be made differentiable by an interpreter. \n",
    "\n",
    "We could generalizing formalize this notion using Turing machines and differentiating operations on the tape, but this wouldn't be a very useful kind of differentiation in practice. In most cases, automatic differentiation is defined over a subset of a common programming language or library, or for a domain-specific language. By restricting the domain of AD to operations for which we can define efficient derivative operations, we gain a lot of expressive power and efficiency. An example is [`autograd`](https://github.com/HIPS/autograd), which replaces large portions of `numpy` and `scipy` with differentiable operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lsps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the mechanics of AD, we are going to build several languages.\n",
    "\n",
    "Each of these languages is going to a \"lsp\" - or not quite a Lisp. Don't be fooled: they are missing some key features that most Lisps have. They aren't even all Turing complete. Lisp syntax is used, because it's easy to parse, and parsing is not the main interest here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each lsp is defined by the tuple $l = <\\mathcal V, \\mathcal O, \\mathcal A, \\mathcal E>$ which includes:\n",
    "- A value type $V$, the set of valid computational results in the language.\n",
    "- An operator type $\\mathcal O$, the set of valid transformations $F: \\mathcal V \\to \\mathcal V$ \n",
    "- An atomizer $\\mathcal A:\\mathcal S \\to \\mathcal V \\cup \\mathcal O$, converting atomic tokens from the set of valid  tokens $\\mathcal S$ (generally a set of strings without parens or whitespace) into value and operator type instances\n",
    "- An environment $\\mathcal E$ that contains predefined items in $\\mathcal V \\cup \\mathcal O$. By default the environment is static, but it could be made dynamic in some lsps (though not all of those defined in this article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax rules are very similar to standard Lisp:\n",
    "\n",
    "1. f the atomizer accepts an expression, that expression is valid.\n",
    "\n",
    "1. If an expression $e$\n",
    "    - consists of a pair of matched parens with at least two subexpressions `(a, b ...)` separated by whitespace, and \n",
    "    - every subexpression $e$ contains is valid, and \n",
    "    - the first subexpression of $e$, `a`, evaluates to an operator type for this lsp, \n",
    "    \n",
    "   then the whole expression is valid.\n",
    "\n",
    "All other expressions are invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse a lsp quite easily, by tokenizing the input string and recursively processing parentheses contents.\n",
    "Parsing a tokenized string consists of converting all tokens except parens to the lsp's operator and value types, and then using the paren structure to create a nested list with identical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "import re\n",
    "import math\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lsp(object):\n",
    "    open_paren = re.compile(\"\\(\")\n",
    "    close_paren = re.compile(\"\\)\")\n",
    "    splitter = re.compile(\"\\s+\")\n",
    "\n",
    "    def __init__(self, value_type, operator_type, atomizer, environment):\n",
    "        self.value_type = value_type\n",
    "        self.operator_type = operator_type\n",
    "        self.atomizer = atomizer\n",
    "        self.environment = environment\n",
    "        \n",
    "    def parse_helper(self, tokens):\n",
    "        token = tokens.pop(0)\n",
    "        if token == \"(\":\n",
    "            els = []\n",
    "            while tokens[0] != \")\":\n",
    "                next_expr, tokens = self.parse_helper(tokens)\n",
    "                els += [next_expr]\n",
    "\n",
    "            return els, tokens[1:]\n",
    "        elif token == \")\":\n",
    "            raise ValueError()\n",
    "        else:\n",
    "            return self.atomize(token), tokens\n",
    "        \n",
    "    def parse(self, tokens):\n",
    "        parsed, _ = self.parse_helper(tokens)\n",
    "        return parsed\n",
    "    \n",
    "    def atomize(self, token):\n",
    "        if token in self.environment.keys():\n",
    "            return self.environment[token]\n",
    "        return self.atomizer(token)\n",
    "    \n",
    "    def tokenize(self, code_str):\n",
    "        return list(filter(\n",
    "            lambda x: x is not '', \n",
    "            Lsp.splitter.split(Lsp.close_paren.sub(\" ) \", Lsp.open_paren.sub(\" ( \", code_str)))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate an lsp expression by recursively evaluating the subexpressions from the \"bottom up\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalLsp(Lsp):\n",
    "    \"\"\"A lisp that may be evaluated.\"\"\"\n",
    "    def eval_stack(self, stack):\n",
    "        if isinstance(stack, self.value_type) or isinstance(stack, self.operator_type):\n",
    "            # atom\n",
    "            return stack\n",
    "\n",
    "        fn = stack[0]\n",
    "\n",
    "        if isinstance(fn, list):\n",
    "            # nested function\n",
    "            return self.eval_stack(self.eval_stack(stack[0]) + stack[1:])\n",
    "\n",
    "        if isinstance(fn, self.operator_type):\n",
    "            # eval this function on the args\n",
    "            return fn(*map(self.eval_stack, stack[1:]))\n",
    "    \n",
    "        raise ValueError(f\"Unparsable value {fn}\")\n",
    "        \n",
    "    def eval_str(self, code_str):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lsplsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lsplsp` is about the simplest lsp langauge that we can make from this definition that isn't degenerate. It can run a few functions on Python ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLsp(EvalLsp):\n",
    "    \"\"\"A DoubleLsp is an Lsp that returns the same value in the forward and backward results.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (evaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LspLspValue = int\n",
    "class LspLspOperator(object):\n",
    "    \"\"\"An lsplsp operator is just a Python function annotated with a symbol.\"\"\"\n",
    "    def __init__(self, symbol, function):\n",
    "        self.symbol = symbol\n",
    "        self.function = function\n",
    "    def __call__(self, *args):\n",
    "        return self.function(*args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_token = re.compile(\"^\\d+$\")\n",
    "\n",
    "def lsplsp_atomizer(atom):\n",
    "    \"\"\"Lsplsp attempts to evaluate anything it can't find in it's environment as a Python int.\"\"\"\n",
    "    if int_token.match(atom):\n",
    "        return int(atom)\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsplsp_sum = LspLspOperator('+', lambda *args: sum(args))\n",
    "lsplsp_diff = LspLspOperator('-', lambda *args: args[0] - sum(args[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsplsp = SimpleLsp(\n",
    "    LspLspValue,\n",
    "    LspLspOperator,\n",
    "    lsplsp_atomizer,\n",
    "    { f.symbol: f for f in [lsplsp_sum, lsplsp_diff] }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsplsp.eval_str(\"(+ 5 2 (- 6 3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-backward lsps\n",
    "\n",
    "The rest of the lsps we're going to take a look at are \"forward-backward lsps,\" or fb-lsps for short.\n",
    "\n",
    "There are two special characteristics of fb-lsps: \n",
    "\n",
    "1. Evaluation of an fb-lsp expression returns all the intermediate results in a nested structure.\n",
    "1. Evaluation of an fb-lsp expression results in two results: a 'forward' results and a 'backward' result.\n",
    "\n",
    "Exactly what these two results will be used for is an inscrutable mystery for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recycle `lsplsp` as `fblsplsp`, an fb-lsp that simply returns the same value in F and B results. \n",
    "It's cheating a little to call this an fb-lsp, and it doesn't provide us with anything new or useful, but we'll do it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBCheatLsp(EvalLsp):\n",
    "    \"\"\"An Lsp that that returns the same value in the forward and backward results.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (\n",
    "            evaled,\n",
    "            evaled\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FBLspLspValue = int\n",
    "\n",
    "class FBLspLspOperator(object):\n",
    "    \"\"\"An fblsplsp function returns the argument values in a tuple with the result.\"\"\"\n",
    "    def __init__(self, symbol, function):\n",
    "        self.symbol = symbol\n",
    "        self.function = function\n",
    "    def __call__(self, *args):\n",
    "        return (self.function(*(arg if isinstance(arg, FBLspLspValue) else arg[0] for arg in args)), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fblsplsp_sum = FBLspLspOperator('+', lambda *args: sum(args))\n",
    "fblsplsp_diff = FBLspLspOperator('-', lambda *args: args[0] - sum(args[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fblsplsp = FBCheatLsp(\n",
    "    FBLspLspValue,\n",
    "    FBLspLspOperator,\n",
    "    lsplsp_atomizer,\n",
    "    { f.symbol: f for f in [fblsplsp_sum, fblsplsp_diff] }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate an `fblsplsp` expression, we get a tuple of the forward and backwards results. Each element of the tuple is a nested tuple, forming a tree of intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((7, (5, 2, (0, (6, 3, 3)))), (7, (5, 2, (0, (6, 3, 3)))))\n"
     ]
    }
   ],
   "source": [
    "pprint(fblsplsp.eval_str(\"(+ 5 2 (- 6 3 3))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, our fb-lsps will compute two different values for the forward and backward result, with some shared computation. We can implement these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBHookLsp(EvalLsp):\n",
    "    \"\"\"An Lsp that computes the forward and backward results using a hook from stack evaluation.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (\n",
    "            evaled.forward(),\n",
    "            evaled.backward()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gradlsp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get back on topic: automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradlsp` is an fb-lsp that returns a graph of conventional computation (just like `fblsplsp`) in the forward result, and returns a graph of gradients taken with respect to the final result in the backward result. We will define `gradlsp` to work over the domain of real numbers (approximated as Python floating-point values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_token = re.compile(\"^\\d+$\")\n",
    "float_token = re.compile(\"^\\d+(\\.\\d*)$\")\n",
    "#string_token = re.compile(\"^\\\"([^\\\"]*)\\\"\")\n",
    "\n",
    "def gradlsp_atomizer(token):\n",
    "    if int_token.match(token):\n",
    "        return GradLspValue(float(token))\n",
    "    if float_token.match(token):\n",
    "        return  GradLspValue(float(token))\n",
    "    raise ValueError(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradlsp` implements \"reverse-mode\" automatic differentiation. \n",
    "Any operation in `gradlsp` that takes arguments $a$ and returns a value $v$ records, in $v$, \n",
    "the necessary information to compute the gradients of each of $a$ (and $a$'s ancestor arguments) with respect to $v$ or a value computed by operations on $v$. \n",
    "\n",
    "The information stored in each value $v$ is the complete graph of previous values leading to $v$'s computation, as well as the gradient of the function $f$ that produced $v$ at the point of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, gradient computation of all subresults from one of these graphs requires a (computationally expensive) topological sort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `gradlsp` doesn't have any variables. Every operation on a value in gradlsp is pure with respect to the boxed value - it does not modify the original value. Furthermore, all operations are pure with respect to the value's gradient graph. Even the detach operation simply returns a new value with a degenerate graph. This ensures that the gradient graph is a DAG: a cycle in the graph would require a changed (impure) value. Because there are no cycles, we can treat the gradient graph as if it were a tree. This is very inefficient: if the DAG isn't treelike, we are performing wasted computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradLspValue(object):\n",
    "    def __init__(self, value, gradient = [], children = [], symbol = None, operator_symbol=None):\n",
    "        self.gradient = gradient\n",
    "        self.children = children\n",
    "        self.value = value\n",
    "        self.symbol = symbol\n",
    "        self.operator_symbol = operator_symbol\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<GLV {self.value}, gradient={self.gradient}, with {len(self.children)} children>\"\n",
    "        \n",
    "    def forward(self):\n",
    "        return (self.value, self.operator_symbol or '?') + ((\n",
    "            [child.forward() for child in self.children],\n",
    "        ) if len(self.children) > 0 else tuple())\n",
    "    \n",
    "    def backward(self, adjoint=1):\n",
    "        if len(self.children) == 0:\n",
    "            return adjoint\n",
    "        return (adjoint, self.operator_symbol or '?') + ((\n",
    "            [child.backward(adjoint * derivative) \n",
    "             for child, derivative in zip(self.children, self.gradient)],\n",
    "        ) if len(self.children) > 0 else tuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, in `gradlsp`, we require every operator be scalar-valued. For this reason, the compute DAG for any value is by necessity a tree: every operation has only one parent. This decision reduces the power of `gradlsp`, as we will see soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradLspOperator(object):\n",
    "    def __init__(self, symbol, function, grad_function):\n",
    "        self.function = function\n",
    "        self.grad_function = grad_function\n",
    "        self.symbol = symbol\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<GLF {self.symbol}>\"\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        result = GradLspValue(\n",
    "            self.function(*map(lambda glv: glv.value, args)),\n",
    "            self.grad_function(*map(lambda glv: glv.value, args)),\n",
    "            args,\n",
    "            operator_symbol=self.symbol\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the backward result, we propagate the gradient of the final value $y$ backwards. This consists of repeated vector multiplication between the original value and the intermediate gradients. At each intermediate value $v$, we obtain an adjoint value $\\frac{dy}{dv}$, the derivative of $v$ with respect to the final result $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus = GradLspOperator(\n",
    "    '+',\n",
    "    lambda *args: sum(args),\n",
    "    lambda *args: [1,] * len(args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus = GradLspOperator(\n",
    "    '-',\n",
    "    lambda *args: args[0] - sum(args[1:]),\n",
    "    lambda *args: [1,] + [-1,] * (len(args) - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = GradLspOperator(\n",
    "    '*',\n",
    "    lambda *args: reduce(operator.mul, args, 1),\n",
    "    lambda *args: [reduce(operator.mul, (args[:i] + args[i+1:]), 1) for i in range(len(args))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quot = GradLspOperator(\n",
    "    '/',\n",
    "    lambda x, y: x / y,\n",
    "    lambda x, y: [1/y, -(x/(math.pow(y, 2)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = GradLspOperator(\n",
    "    'sin',\n",
    "    lambda x: math.sin(x),\n",
    "    lambda x: [math.cos(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = GradLspOperator(\n",
    "    'cos',\n",
    "    lambda x: math.cos(x),\n",
    "    lambda x: [-math.sin(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "detach = GradLspOperator(\n",
    "    '$',\n",
    "    lambda x: x,\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detach operation `($ foo)` detaches the compute graph of `foo` from the returned result (which has the same *value* as foo), resulting in a new compute graph consisting of `foo` only.\n",
    "The (singular) node of the gradient DAG rooted at `value`, in any case, will be zero.\n",
    "This value, though it may depend on other values in actuality, is \"held constant\" from the standpoint of the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = GradLspValue(math.pi, symbol='pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradlsp = FBHookLsp(\n",
    "    GradLspValue, \n",
    "    GradLspOperator, \n",
    "    gradlsp_atomizer, \n",
    "    { f.symbol: f for f in [plus, minus, prod, quot, sin, cos, pi, detach] }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-6.0,\n",
      "  '*',\n",
      "  [(6.0, '*', [(3.0, '?'), (2.0, '?')]),\n",
      "   (-1.0, 'cos', [(3.141592653589793, '?')])]),\n",
      " (1, '*', [(-1.0, '*', [-2.0, -3.0]), (6.0, 'cos', [-7.347880794884119e-16])]))\n"
     ]
    }
   ],
   "source": [
    "pprint(gradlsp.eval_str(\"(* (* 3 2) (cos pi))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jacoblsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradlsp` isn't very useful. \n",
    "It works on scalar valued functions only. \n",
    "It's hard to implement more than a best fit line with it.\n",
    "\n",
    "`jacoblsp` is our next step.\n",
    "Values in `jacoblsp` are tensors of arbitrary size (though many functions only accept 1D or 2D tensors).\n",
    "\n",
    "An operator in `jacoblsp` can express a function $\\mathbb R^n \\to \\mathbb R^m$.\n",
    "Correspondingly, instead of the gradient, `jacoblsp` computes the Jacobian tensor for each operation.\n",
    "Instead of storing the local Jacobian itself in each value, and multiplying by the appropriate adjoint in the backward pass, we store a function that takes the adjoint and returns adjoint matrices for each of the generating function's arguments. This allows us to minimize space use in some operators, and concisely express others. \n",
    "\n",
    "Each value in `jacoblsp` is backed by an n-dimensional `numpy` array.\n",
    "An array is expressed as a literal `1,2,3`. Trailing commas are allowed, but spaces are not.\n",
    "For instance, `(*. 1,2,3 4,5,6)`  will compute the dot product of $[1,2,3]$ with $[4,5,6]$\n",
    "Note that every value in `jacoblsp` is an array: scalar values are simply arrays of size one.\n",
    "\n",
    "Note that `jacoblsp` is still very inefficient. We compute every subtree of the compute DAG for every backwards operation. If our DAG isn't treelike, we are performing wasted computations. It's easier to express combinatorically complex programs in `jacoblsp` than it was in `jacoblsp`, so this problem is more worrying. However, we still have a treelike DAG, assuming that individual gradient functions do not branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobLspValue(object):\n",
    "    def __init__(self, value, adjoint_fns = None, children=[], shape=None, symbol = None, operator_symbol=None):\n",
    "        self.adjoint_fns = adjoint_fns if adjoint_fns else [(lambda x: x) for c in children]\n",
    "        self.children = children\n",
    "        self.value = value\n",
    "        self.symbol = symbol\n",
    "        self.operator_symbol = operator_symbol\n",
    "        self.shape = shape if shape else self.value.shape\n",
    "    def __repr__(self):\n",
    "        return f\"<JLV {self.value}, with {len(self.children)} children>\"\n",
    "    \n",
    "    def forward(self):\n",
    "        if not self.children:\n",
    "            return (self.value,)\n",
    "        return (\n",
    "            self.value, \n",
    "            self.operator_symbol or '?', \n",
    "            [child.forward() for child in self.children]\n",
    "        )\n",
    "    \n",
    "    def backward(self, adjoint=None):\n",
    "        if adjoint is None:\n",
    "            adjoint = self.value\n",
    "        \n",
    "        if not self.children:\n",
    "            return (adjoint,)\n",
    "        \n",
    "        return (\n",
    "            adjoint, \n",
    "            self.operator_symbol or '?', \n",
    "            [child.backward(adjoint_fn(adjoint)) \n",
    "             for adjoint_fn, child \n",
    "             in zip(self.adjoint_fns, self.children)\n",
    "            ]\n",
    "        )\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobLspOperator(object):\n",
    "    def __init__(self, symbol, function, max_args=None, max_dims=None):\n",
    "        self.function = function\n",
    "        self.symbol = symbol\n",
    "        self.max_args = max_args\n",
    "        self.max_dims = max_dims\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<JLF {self.symbol}>\"\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        if self.max_args is not None and len(args) > self.max_args:\n",
    "            raise ArgumentException()\n",
    "        if self.max_dims and any(np.ndim(arg) > max_dim for arg, max_dim in zip(args, self.max_dims)):\n",
    "            raise ArgumentException()\n",
    "        arg_values = list(map(lambda jlv: jlv.value, args))\n",
    "        output_value, adjoint_fns = self.function(*arg_values)\n",
    "        result = JacobLspValue(\n",
    "            output_value,\n",
    "            adjoint_fns,\n",
    "            args if adjoint_fns else [],\n",
    "            operator_symbol=self.symbol\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_arr_token= re.compile(\"^-?(\\d+,)*-?\\d+,?$\")\n",
    "float_arr_token= re.compile(\"^(-?\\d+(\\.\\d*)?,)*-?\\d+(\\.\\d*)?,?$\")\n",
    "\n",
    "def jacoblsp_atomizer(token):\n",
    "    if int_arr_token.match(token):\n",
    "        return JacobLspValue(np.array(list(map(int, filter(bool, token.split(',')))), dtype='int'))\n",
    "    if int_arr_token.match(token):\n",
    "        return JacobLspValue(np.array(list(map(float, filter(bool, token.split(',')))), dtype='float'))\n",
    "    raise ValueError(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jplus = JacobLspOperator(\n",
    "    '+',\n",
    "    lambda *args: (\n",
    "        np.stack(args).sum(axis=0),\n",
    "        [(lambda g: g) for a in args]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jminus = JacobLspOperator(\n",
    "    '-',\n",
    "    lambda *args: (\n",
    "        args[0] - np.stack(args[1:]).sum(axis=0),\n",
    "        [lambda g: g] + [(lambda g: -g) for a in args[1:]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jprod = JacobLspOperator(\n",
    "    '*',\n",
    "    lambda *args: (\n",
    "        np.stack(args).prod(axis=0),\n",
    "        [(lambda g: g.dot(np.stack(args[:i] + args[i+1:]))) for i in range(len(args))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matvecdot = (lambda A, x: [\n",
    "    lambda g: np.tensordot(g, x, 0),\n",
    "    lambda g: np.tensordot(g, A, [-1, 0])\n",
    "])\n",
    "\n",
    "vecmatdot = (lambda a, B: [\n",
    "    lambda g: np.tensordot(g, B.T, 1),\n",
    "    lambda g: np.tensordot(g, a, 0)\n",
    "])\n",
    "\n",
    "vecvecdot = (lambda a, b: [\n",
    "    lambda g: b,\n",
    "    lambda g: a,\n",
    "])\n",
    "\n",
    "matmatdot = (lambda A, B: [\n",
    "    lambda g: np.tensordot(g, B.T, 1),\n",
    "    lambda g: np.tensordot(g, A, [-2, 0]).T\n",
    "])\n",
    "\n",
    "selectdot = ({\n",
    "    (1,1): vecvecdot,\n",
    "    (2,1): matvecdot,\n",
    "    (1,2): vecmatdot,\n",
    "    (2,2): matmatdot,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdot = JacobLspOperator(\n",
    "    '*.',\n",
    "    lambda A, B: (\n",
    "        np.dot(A, B),\n",
    "        selectdot[A.ndim, B.ndim](A,B)\n",
    "    ),\n",
    "    max_args = 2,\n",
    "    max_dims = [2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "jquot = JacobLspOperator(\n",
    "    '/',\n",
    "    lambda x, y: (\n",
    "        x / y,\n",
    "        [lambda g: g/y, lambda g: g.dot(-(x/(math.pow(y, 2))))],\n",
    "    ),\n",
    "    max_args = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcos = JacobLspOperator(\n",
    "    'cos',\n",
    "    lambda x: (\n",
    "        np.cos(x),\n",
    "        [lambda g: g.dot(-np.sin(x))]\n",
    "    ),\n",
    "    max_args = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsin = JacobLspOperator(\n",
    "    'sin',\n",
    "    lambda x: (\n",
    "        np.sin(x),\n",
    "        [lambda g: g.dot(np.cos(x))],\n",
    "    ),\n",
    "    max_args = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfill = JacobLspOperator(\n",
    "    'full',\n",
    "    lambda fill, *shp: (\n",
    "        np.full(shp[0] if len(shp) == 1 else shp, fill),\n",
    "        None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = JacobLspOperator(\n",
    "    'relu',\n",
    "    lambda x: (\n",
    "        np.where(x < 0, 0, x),\n",
    "        [lambda g: (g * np.where(x < 0, 0, 1)),],\n",
    "    ),\n",
    "    max_args=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = JacobLspOperator(\n",
    "    'stack',\n",
    "    lambda *args: (np.stack(args), [lambda g: g[i] for i in range(len(args))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = JacobLspOperator(\n",
    "    'T',\n",
    "    lambda arg: (arg.T, [lambda g: g.T]) if len(arg.shape) == 2 else (arg.reshape((1,-1)), [lambda g: g.reshape(1,-1)]),\n",
    "    max_args = 1,\n",
    "    max_dims = [2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacoblsp = FBHookLsp(\n",
    "    JacobLspValue,\n",
    "    JacobLspOperator,\n",
    "    jacoblsp_atomizer,\n",
    "    { f.symbol: f for f in [jplus, jminus, jprod, jquot, jsin, jcos, jdot, jfill, relu, stack, transpose] }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the building blocks to implement a neural network in `jacoblsp`, and take the network's gradient.\n",
    "\n",
    "As an example, we can build a classification network, which will compute the binary XOR of two inputs (with an appended bias term). It will consist of two layers with a single nonlinear activation. Using the inputs $(1,1), (0,1), (1,0), (0,0)$, we would expect to obtain the outputs $0, 1, 1, 0$. This is a toy problem, for which layer weights can be given a priori, with no training necessary. The network is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[0, 1, 1, 0]]),\n",
      "  '*.',\n",
      "  [(array([[-2,  1]]), 'stack', [(array([-2,  1]),)]),\n",
      "   (array([[1, 0, 0, 0],\n",
      "       [2, 1, 1, 0]]),\n",
      "    'relu',\n",
      "    [(array([[ 1,  0,  0, -1],\n",
      "       [ 2,  1,  1,  0]]),\n",
      "      '*.',\n",
      "      [(array([[ 1,  1, -1],\n",
      "       [ 1,  1,  0]]),\n",
      "        'stack',\n",
      "        [(array([ 1,  1, -1]),), (array([1, 1, 0]),)]),\n",
      "       (array([[1, 0, 1, 0],\n",
      "       [1, 1, 0, 0],\n",
      "       [1, 1, 1, 1]]),\n",
      "        'T',\n",
      "        [(array([[1, 1, 1],\n",
      "       [0, 1, 1],\n",
      "       [1, 0, 1],\n",
      "       [0, 0, 1]]),\n",
      "          'stack',\n",
      "          [(array([1, 1, 1]),),\n",
      "           (array([0, 1, 1]),),\n",
      "           (array([1, 0, 1]),),\n",
      "           (array([0, 0, 1]),)])])])])]),\n",
      " (array([[0, 1, 1, 0]]),\n",
      "  '*.',\n",
      "  [(array([[0, 2]]), 'stack', [(array([0, 2]),)]),\n",
      "   (array([[ 0, -2, -2,  0],\n",
      "       [ 0,  1,  1,  0]]),\n",
      "    'relu',\n",
      "    [(array([[ 0, -2, -2,  0],\n",
      "       [ 0,  1,  1,  0]]),\n",
      "      '*.',\n",
      "      [(array([[-2, -2, -4],\n",
      "       [ 1,  1,  2]]),\n",
      "        'stack',\n",
      "        [(array([1, 1, 2]),), (array([1, 1, 2]),)]),\n",
      "       (array([[ 0, -1, -1,  0],\n",
      "       [ 0, -1, -1,  0],\n",
      "       [ 0,  2,  2,  0]]),\n",
      "        'T',\n",
      "        [(array([[ 0,  0,  0],\n",
      "       [-1, -1,  2],\n",
      "       [-1, -1,  2],\n",
      "       [ 0,  0,  0]]),\n",
      "          'stack',\n",
      "          [(array([0, 0, 0]),),\n",
      "           (array([0, 0, 0]),),\n",
      "           (array([0, 0, 0]),),\n",
      "           (array([0, 0, 0]),)])])])])]))\n"
     ]
    }
   ],
   "source": [
    "pprint(jacoblsp.eval_str('(*. (stack -2,1) (relu (*. (stack 1,1,-1 1,1,0) (T (stack 1,1,1 0,1,1 1,0,1 0,0,1)))))'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `jacoblsp` has computed the correct result and a gradient for it with each of the intermediate functions. In particular, we can see that it has computed the gradients for the layer weights. Appending a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[0, 0, 0, 0]]),\n",
      "  '-',\n",
      "  [(array([[0, 1, 1, 0]]), 'T', [(array([0, 1, 1, 0]),)]),\n",
      "   (array([[0, 1, 1, 0]]),\n",
      "    '*.',\n",
      "    [(array([[-2,  1]]), 'stack', [(array([-2,  1]),)]),\n",
      "     (array([[1, 0, 0, 0],\n",
      "       [2, 1, 1, 0]]),\n",
      "      'relu',\n",
      "      [(array([[ 1,  0,  0, -1],\n",
      "       [ 2,  1,  1,  0]]),\n",
      "        '*.',\n",
      "        [(array([[ 1,  1, -1],\n",
      "       [ 1,  1,  0]]),\n",
      "          'stack',\n",
      "          [(array([ 1,  1, -1]),), (array([1, 1, 0]),)]),\n",
      "         (array([[1, 0, 1, 0],\n",
      "       [1, 1, 0, 0],\n",
      "       [1, 1, 1, 1]]),\n",
      "          'T',\n",
      "          [(array([[1, 1, 1],\n",
      "       [0, 1, 1],\n",
      "       [1, 0, 1],\n",
      "       [0, 0, 1]]),\n",
      "            'stack',\n",
      "            [(array([1, 1, 1]),),\n",
      "             (array([0, 1, 1]),),\n",
      "             (array([1, 0, 1]),),\n",
      "             (array([0, 0, 1]),)])])])])])]),\n",
      " (array([[0, 0, 0, 0]]),\n",
      "  '-',\n",
      "  [(array([[0, 0, 0, 0]]), 'T', [(array([[0, 0, 0, 0]]),)]),\n",
      "   (array([[0, 0, 0, 0]]),\n",
      "    '*.',\n",
      "    [(array([[0, 0]]), 'stack', [(array([0, 0]),)]),\n",
      "     (array([[0, 0, 0, 0],\n",
      "       [0, 0, 0, 0]]),\n",
      "      'relu',\n",
      "      [(array([[0, 0, 0, 0],\n",
      "       [0, 0, 0, 0]]),\n",
      "        '*.',\n",
      "        [(array([[0, 0, 0],\n",
      "       [0, 0, 0]]),\n",
      "          'stack',\n",
      "          [(array([0, 0, 0]),), (array([0, 0, 0]),)]),\n",
      "         (array([[0, 0, 0, 0],\n",
      "       [0, 0, 0, 0],\n",
      "       [0, 0, 0, 0]]),\n",
      "          'T',\n",
      "          [(array([[0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]]),\n",
      "            'stack',\n",
      "            [(array([0, 0, 0]),),\n",
      "             (array([0, 0, 0]),),\n",
      "             (array([0, 0, 0]),),\n",
      "             (array([0, 0, 0]),)])])])])])]))\n"
     ]
    }
   ],
   "source": [
    "pprint(jacoblsp.eval_str('(- (T 0,1,1,0) (*. (stack -2,1) (relu (*. (stack 1,1,-1 1,1,0) (T (stack 1,1,1 0,1,1 1,0,1 0,0,1))))))'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the gradient is zero, because the loss is zero. Perturbing the layer weights results in nonzero gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how the operators of a language may be incrementally extended to include gradient operators, and how this results in an automatically differentiable language. In the follow-up to this article, I will show how `jacoblsp` can be extended to include gradient descent with learnable parameters, and explore how to compute efficient gradients on non-treelike compute graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
