{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation isn't magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent results in deep and convolutional neural networks with powerful industrial applications have led to widespread adoption of these techniques outside of academia. One key advance has been the introduction of automatic differentiation, which allows developers and scientists to write code using standard numerical routines and functions that can be differentiated and used in training neural networks or other machine learning models. Many developers working with neural networks today, however, do not have a strong understanding of how automatic differentiation functions \"under the hood,\" as well as how it differs from symbolic and numerical differentiation. In this article, using the style of literate programming, I will build some toy domain-specific languages that model how automatic differentiation functions in popular libraries like Tensorflow and Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please note that this is not a primer on automatic differentiation itself or the underlying theory. There are many high-quality sources on these topics, which I will not hope to outdo. The purpose of this article is to show, to those familiar with the concepts of AD but not the implementation, how simply automatic differentiation can be implemented. \n",
    "\n",
    "For this reason I will present minimal background on AD and the alternatives and invite readers to search out these topics themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are roughly three ways to perform mathematical differentiation that most people are aware of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Symbolic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the application of rules on abstract symbols to analytically calculate formulas for the derivative of $f$ at $x$ (where $x$ may be high dimensional). \n",
    "\n",
    "Symbolic differentiation involves a *computer algebra system* (CAS). Some people differentiate between symbolic differentiation and \"manual\" differentiation, where the latter is the same essential procedure as the former, but is performed, generally on paper, by a human.\n",
    "\n",
    "In practice, this would be something like a CAS looking up a rule in a table to convert an expression $y = x^2$, consisting of exponentiation of a scalar value, to a derivative expression $y' = 2x$, without ever considering the value of x itself. Often, this would occur without even considering the value of *2*. Frege, at that point, is rolling in his grave. Alternatively, this would consist of a very bored 16-year-old sitting in math class looking up this rule in a table to generate the second expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the computation of an approximate derivative of $f$ at $x$ using numerical algorithms. \n",
    "\n",
    "The most common techniques are forms of finite difference estimation, which consists of perturbing the value of $x$ and analyzing the resultant change in $f(x)$. Again, this can be done by massive linear algebra algorithms, or bored 16-year-olds.\n",
    "\n",
    "While this technique seems hacky, it's commonly used in engineering and science. It's important to keep in mind that we are working with floating point numbers of finite precision in most cases, and sometimes a \"hacky\" method can max out our precision. At the same time, numerical differentiation requires significant attention to floating point arithmetic and error explosion.\n",
    "\n",
    "Here, the derivative is computed from concrete values, and we do not obtain any structured expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "This is the computation of an exact (to machine arithmetic precision) derivative of $f$ at $x$ by analyzing the *computation* of $f(x)$. Rather than analyzing symbols that express mathematical operations (as in symbolic differentiation), or perturbing computations that represent mathematical operations, we analyze a program that computes a function $f$. This analysis is far simpler than that of symbolic algebra expressions, and can be formalized into predictable algorithms and procedures.\n",
    "\n",
    "The concept behind automatic differentiation is simple: every operation that our program is allowed to perform is shadowed by an operation that can compute the derivative of that operation. In practice, we record operations on values, and then use the \"shadow\" operations to compute derivatives of these operations using the chain rule. You can imagine all of the functions in your programming environment being doubled in this way. With the right abstraction layer, a normally written program can be made differentiable by an interpreter. \n",
    "\n",
    "We could generalizing formalize this notion using Turing machines and differentiating operations on the tape, but this wouldn't be a very useful kind of differentiation in practice. In most cases, automatic differentiation is defined over a subset of a common programming language or library, or for a domain-specific language. By restricting the domain of AD to operations for which we can define efficient derivative operations, we gain a lot of expressive power and efficiency. An example is [`autograd`](https://github.com/HIPS/autograd), which replaces large portions of `numpy` and `scipy` with differentiable operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lsps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To demonstrate the mechanics of AD, we are going to build several languages.\n",
    "\n",
    "Each of these languages is going to a \"lsp\" - or not quite a Lisp. Don't be fooled: they are missing some key features that most Lisps have. They aren't even all Turing complete. Lisp syntax is used, because it's easy to parse, and parsing is not the main interest here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each lsp is defined by the tuple $l = <\\mathcal V, \\mathcal O, \\mathcal A, \\mathcal E>$ which includes:\n",
    "- A value type $V$, the set of valid computational results in the language.\n",
    "- An operator type $\\mathcal O$, the set of valid transformations $F: \\mathcal V \\to \\mathcal V$ \n",
    "- An atomizer $\\mathcal A:\\mathcal S \\to \\mathcal V \\cup \\mathcal O$, converting atomic tokens from the set of valid  tokens $\\mathcal S$ (generally a set of strings without parens or whitespace) into value and operator type instances\n",
    "- An environment $\\mathcal E$ that contains predefined items in $\\mathcal V \\cup \\mathcal O$. By default the environment is static, but it could be made dynamic in some lsps (though not all of those defined in this article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The syntax rules are very similar to standard Lisp:\n",
    "\n",
    "1. f the atomizer accepts an expression, that expression is valid.\n",
    "\n",
    "1. If an expression $e$\n",
    "    - consists of a pair of matched parens with at least two subexpressions `(a, b ...)` separated by whitespace, and \n",
    "    - every subexpression $e$ contains is valid, and \n",
    "    - the first subexpression of $e$, `a`, evalutes to an operator type for this lsp, \n",
    "    \n",
    "   then the whole expression is valid.\n",
    "\n",
    "All other expressions are invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can parse a lsp quite easily, by tokenizing the input string and recursively processing parentheses contents.\n",
    "Parsing a tokenized string consists of converting all tokens except parens to the lsp's operator and value types, and then using the paren structure to create a nested list with identical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Lsp(object):\n",
    "    open_paren = re.compile(\"\\(\")\n",
    "    close_paren = re.compile(\"\\)\")\n",
    "    splitter = re.compile(\"\\s+\")\n",
    "\n",
    "    def __init__(self, value_type, operator_type, atomizer, environment):\n",
    "        self.value_type = value_type\n",
    "        self.operator_type = operator_type\n",
    "        self.atomizer = atomizer\n",
    "        self.environment = environment\n",
    "        \n",
    "        def parse_helper(self, tokens):\n",
    "        token = tokens.pop(0)\n",
    "        if token == \"(\":\n",
    "            els = []\n",
    "            while tokens[0] != \")\":\n",
    "                next_expr, tokens = self.parse_helper(tokens)\n",
    "                els += [next_expr]\n",
    "\n",
    "            return els, tokens[1:]\n",
    "        elif token == \")\":\n",
    "            raise ValueError()\n",
    "        else:\n",
    "            return self.atomize(token), tokens\n",
    "        \n",
    "    def parse(self, tokens):\n",
    "        parsed, _ = self.parse_helper(tokens)\n",
    "        return parsed\n",
    "    \n",
    "    def atomize(self, token):\n",
    "        if token in self.environment.keys():\n",
    "            return self.environment[token]\n",
    "        return self.atomizer(token)\n",
    "    \n",
    "    def tokenize(self, code_str):\n",
    "        return list(filter(\n",
    "            lambda x: x is not '', \n",
    "            Lsp.splitter.split(Lsp.close_paren.sub(\" ) \", Lsp.open_paren.sub(\" ( \", code_str)))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can evaluate an lsp expression by recursively evaluating the subexpressions \"bottom up\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EvalLsp(Lsp):\n",
    "    \"\"\"A lisp that may be evaluated.\"\"\"\n",
    "    def eval_stack(self, stack):\n",
    "        if isinstance(stack, self.value_type) or isinstance(stack, self.operator_type):\n",
    "            # atom\n",
    "            return stack\n",
    "\n",
    "        fn = stack[0]\n",
    "\n",
    "        if isinstance(fn, list):\n",
    "            # nested function\n",
    "            return self.eval_stack(self.eval_stack(stack[0]) + stack[1:])\n",
    "\n",
    "        if isinstance(fn, self.operator_type):\n",
    "            # eval this function on the args\n",
    "            return fn(*map(self.eval_stack, stack[1:]))\n",
    "    \n",
    "        raise ValueError(f\"Unparsable value {fn}\")\n",
    "        \n",
    "    def eval_str(self, code_str):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## lsplsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Lsplsp` is about the simplest lsp langauge that we can get. It can run a few functions on Python ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleLsp(EvalLsp):\n",
    "    \"\"\"A DoubleLsp is an Lsp that returns the same value in the forward and backward results.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (evaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LspLspValue = int\n",
    "class LspLspOperator(object):\n",
    "    \"\"\"An lsplsp operator is just a Python function annotated with a symbol.\"\"\"\n",
    "    def __init__(self, symbol, function):\n",
    "        self.symbol = symbol\n",
    "        self.function = function\n",
    "    def __call__(self, *args):\n",
    "        return self.function(*args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "int_token = re.compile(\"^\\d+$\")\n",
    "\n",
    "def lsplsp_atomizer(atom):\n",
    "    \"\"\"Lsplsp attempts to evaluate anything it can't find in it's environment as a Python int.\"\"\"\n",
    "    if int_token.match(atom):\n",
    "        return int(atom)\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lsplsp_sum = LspLspOperator('+', lambda *args: sum(args))\n",
    "lsplsp_diff = LspLspOperator('-', lambda *args: args[0] - sum(args[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lsplsp = SimpleLsp(\n",
    "    LspLspValue,\n",
    "    LspLspOperator,\n",
    "    lsplsp_atomizer,\n",
    "    { f.symbol: f for f in [lsplsp_sum, lsplsp_diff] }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsplsp.eval_str(\"(+ 5 2 (- 6 3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Forward-backward lsps\n",
    "\n",
    "The rest of the lsps we're going to take a look at are \"forward-backward lsps,\" or fb-lsps for short.\n",
    "\n",
    "There are two special characteristics of fb-lsps: \n",
    "\n",
    "1. Evaluation of an fb-lsp expression returns all the intermediate results in a nested structure.\n",
    "1. Evaluation of an fb-lsp expression results in two results: a 'forward' results and a 'backward' result.\n",
    "\n",
    "Exactly what these two results will be used for is an inscrutable mystery for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can recycle `lsplsp` as `fblsplsp`, an fb-lsp that simply returns the same value in F and B results. \n",
    "It's cheating a little to call this an fb-lsp, and it doesn't provide us with anything new or useful, but we'll do it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FBCheatLsp(EvalLsp):\n",
    "    \"\"\"An Lsp that that returns the same value in the forward and backward results.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (\n",
    "            evaled,\n",
    "            evaled\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FBLspLspValue = int\n",
    "\n",
    "class FBLspLspOperator(object):\n",
    "    \"\"\"An fblsplsp function returns the argument values in a tuple with the result.\"\"\"\n",
    "    def __init__(self, symbol, function):\n",
    "        self.symbol = symbol\n",
    "        self.function = function\n",
    "    def __call__(self, *args):\n",
    "        return (self.function(*(arg if isinstance(arg, FBLspLspValue) else arg[0] for arg in args)), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fblsplsp_sum = FBLspLspOperator('+', lambda *args: sum(args))\n",
    "fblsplsp_diff = FBLspLspOperator('-', lambda *args: args[0] - sum(args[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fblsplsp = FBCheatLsp(\n",
    "    FBLspLspValue,\n",
    "    FBLspLspOperator,\n",
    "    lsplsp_atomizer,\n",
    "    { f.symbol: f for f in [fblsplsp_sum, fblsplsp_diff] }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we evaluate an `fblsplsp` expression, we get a tuple of the forward and backwards results. Each element of the tuple is a nested tuple, forming a tree of intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, (5, 2, (0, (6, 3, 3)))), (7, (5, 2, (0, (6, 3, 3)))))"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fblsplsp.eval_str(\"(+ 5 2 (- 6 3 3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generally, our fb-lsps will compute two different values for the forward and backward result, with some shared computation. We can implement these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FBHookLsp(EvalLsp):\n",
    "    \"\"\"An Lsp that computes the forward and backward results using a hook from stack evaluation.\"\"\"\n",
    "    def eval_str(self, code_str):\n",
    "        stack = self.parse(self.tokenize(code_str))\n",
    "        evaled = self.eval_stack(stack)\n",
    "        return (\n",
    "            evaled.forward(),\n",
    "            evaled.backward()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradlsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get back on topic: automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradlsp is an fb-lsp that returns a graph of conventional computation (just like fblsplsp) in the forward result, and returns a graph of gradients taken with respect to the final result in the backward result. We will define Gradlsp to work over the domain of real numbers (expressed as floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_token = re.compile(\"^\\d+$\")\n",
    "float_token = re.compile(\"^\\d+(\\.\\d*)$\")\n",
    "#string_token = re.compile(\"^\\\"([^\\\"]*)\\\"\")\n",
    "\n",
    "def gradlsp_atomizer(token):\n",
    "    if int_token.match(token):\n",
    "        return GradLspValue(float(token))\n",
    "    if float_token.match(token):\n",
    "        return  GradLspValue(float(token))\n",
    "    raise ValueError(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradlsp implements \"reverse-mode\" automatic differentiation. \n",
    "Any operation in Gradlsp that takes arguments $a$ and returns a value $v$ records, in $v$, \n",
    "the necessary information to compute the gradients of each of $a$ (and $a$'s ancestor arguments) with respect to $v$ or a value computed by operations on $v$. \n",
    "\n",
    "The information stored in each value $v$ is the complete graph of previous values leading to $v$'s computation, as well as the gradient of the function $f$ that produced $v$ at the point of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradLspOperator(GradLspOperator):\n",
    "    def __init__(self, symbol, function, grad_function):\n",
    "        self.function = function\n",
    "        self.grad_function = grad_function\n",
    "        self.symbol = symbol\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<GLF {self.symbol}>\"\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        result = GradLspValue(\n",
    "            self.function(*map(lambda glv: glv.value, args)),\n",
    "            self.grad_function(*map(lambda glv: glv.value, args)),\n",
    "            args\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, gradient computation of all subresults from one of these graphs requires a (computationally expensive) topological sort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `gradlsp` doesn't have any variables. Every operation on a value in gradlsp is pure with respect to the boxed value - it does not modify the original value. Furthermore, all operations are pure with respect to the value's gradient graph. Even the detach operation simply returns a new value with a degenerate graph. This ensures that the gradient graph is a DAG: a cycle in the graph would require a changed (impure) value. Because there are no cycles, we can treat the gradient graph as if it were a tree. This is very inefficient: if the DAG isn't treelike, we are performing wasted computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, in gradlsp, we require every operator be scalar-valued. For this reason, the compute DAG for any value is by necessity a tree: every operation has only one parent. This decision reduce the power of gradlsp, as we will see soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the backward result, we propagate the gradient of the final value $y$ backwards. This consists of repeated vector multiplication between the original value and the intermediate gradients. At each intermediate value $v$, we obtain an `adjoint` value $\\frac{dy}{dv}$, the derivative of $v$ with respect to the final result $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradLspValue(object):\n",
    "    def __init__(self, value, gradient = [], children = [], symbol = None):\n",
    "        self.gradient = gradient\n",
    "        self.children = children\n",
    "        self.value = value\n",
    "        self.symbol = symbol\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<GLV {self.value}, gradient={self.gradient}, with {len(self.children)} children>\"\n",
    "        \n",
    "    def forward(self):\n",
    "        return (self.value,) + ((\n",
    "            [child.forward() for child in self.children],\n",
    "        ) if len(self.children) > 0 else tuple())\n",
    "    \n",
    "    def backward(self, adjoint=1):\n",
    "        if len(self.children) == 0:\n",
    "            return adjoint\n",
    "        return (adjoint,) + ((\n",
    "            [child.backward(adjoint * derivative) \n",
    "             for child, derivative in zip(self.children, self.gradient)],\n",
    "        ) if len(self.children) > 0 else tuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus = GradLspOperatorOperator(\n",
    "    '+',\n",
    "    lambda *args: sum(args),\n",
    "    lambda *args: [1,] * len(args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus = GradLspOperatorOperator(\n",
    "    '-',\n",
    "    lambda *args: args[0] - sum(args[1:]),\n",
    "    lambda *args: [1,] + [-1,] * (len(args) - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = GradLspOperatorOperator(\n",
    "    '*',\n",
    "    lambda *args: reduce(operator.mul, args, 1),\n",
    "    lambda *args: [reduce(operator.mul, (args[:i] + args[i+1:]), 1) for i in range(len(args))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quot = GradLspOperatorOperator(\n",
    "    '/',\n",
    "    lambda x, y: x / y,\n",
    "    lambda x, y: [1/y, -(x/(math.pow(y, 2)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = GradLspOperatorOperator(\n",
    "    'sin',\n",
    "    lambda x: math.sin(x),\n",
    "    lambda x: [math.cos(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = GradLspOperatorOperator(\n",
    "    'cos',\n",
    "    lambda x: math.cos(x),\n",
    "    lambda x: [-math.sin(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "detach = GradLspOperatorOperator(\n",
    "    '$',\n",
    "    lambda x: x,\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detach operation `($ value)` detaches the compute graph of `value` from it, resulting in a new compute graph consisting of `value` only.\n",
    "The (singular) node of the gradient DAG rooted at `value`, in any case, will be zero.\n",
    "This value, though it may depend on other values in actuality, is \"held constant\" from the standpoint of the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = GradLspValue(math.pi, symbol='pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradlsp = FBHookLsp(\n",
    "    GradLspValue, \n",
    "    GradLspOperator, \n",
    "    gradlsp_atomizer, \n",
    "    { f.symbol: f for f in [plus, minus, prod, quot, sin, cos, pi, detach] }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-6.0, [(6, [(3,), (2,)]), (-1.0, [(3.141592653589793,)])]),\n",
       " (1, [(-1.0, [-2.0, -3.0]), (6, [-7.347880794884119e-16])]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradlsp.eval_str(\"(* (* 3 2) (cos pi))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jacoblsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradlsp isn't very useful. \n",
    "It works on scalar valued functions only. \n",
    "It's hard to implement more than a best fit line with it.\n",
    "\n",
    "Jacoblsp is our next step.\n",
    "Values in Jacoblsp are tensors of arbitrary size (though many functions only accept 1D or 2D tensors).\n",
    "\n",
    "An operator in jacoblsp can express a function $\\mathbb R^n \\to \\mathbb R^m$.\n",
    "Correspondingly, instead of the gradient, jacoblsp computes the Jacobian tensor for each operation.\n",
    "Instead of storing the local Jacobian itself in each value, and multiplying by the appropriate adjoint in the backward pass, we store a function that takes the adjoint and returns adjoint matrices for each of the generating function's arguments. This allows us to minimize space use in some operators, and concisely express others. \n",
    "\n",
    "Each value in jacoblsp is backed by an n-dimensional numpy array.\n",
    "An array is expressed as a literal `1,2,3`. Trailing commas are allowed, but spaces are not.\n",
    "For instance, `(*. 1,2,3 4,5,6)`  will compute the dot product of $[1,2,3]$ with $[4,5,6]$\n",
    "Note that every value in jacoblsp is an array: scalar values are simply arrays of size one.\n",
    "\n",
    "Note that Jacoblsp is still very inefficient. We compute every subtree of the compute DAG for every backwards operation. If our DAG isn't treelike, we are performing wasted computations. It's easier to express combinatorically complex programs in jacoblsp than it was in gradlsp, so this problem is more worrying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobLspValue(object):\n",
    "    def __init__(self, value, adjoint_fns = None, children=[], shape=None, symbol = None):\n",
    "        self.adjoint_fns = adjoint_fns if adjoint_fns else [(lambda x: x) for c in children]\n",
    "        self.children = children\n",
    "        self.value = value\n",
    "        self.symbol = symbol\n",
    "        self.shape = shape if shape else self.value.shape\n",
    "    def __repr__(self):\n",
    "        return f\"<JLV {self.value}, with {len(self.children)} children>\"\n",
    "    \n",
    "    def forward(self):\n",
    "        if not self.children:\n",
    "            return (self.value,)\n",
    "        return (self.value, [child.forward() for child in self.children])\n",
    "    \n",
    "    def backward(self, adjoint=None):\n",
    "        if adjoint is None:\n",
    "            adjoint = np.ones(self.value.shape)\n",
    "        \n",
    "        if not self.children:\n",
    "            return (adjoint,)\n",
    "        \n",
    "        # adjoint = (1 x m) where m is the size of this vector\n",
    "            \n",
    "        # self.jacobians = (m x len_i) x len(children) where total size is m x n\n",
    "        return (adjoint, [child.backward(adjoint_fn(adjoint)) for adjoint_fn, child in zip(self.adjoint_fns, self.children)])\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacobLspOperator(object):\n",
    "    def __init__(self, symbol, function, max_args=None, max_dims=None):\n",
    "        self.function = function\n",
    "        self.symbol = symbol\n",
    "        self.max_args = max_args\n",
    "        self.max_dims = max_dims\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"<JLF {self.symbol}>\"\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        if self.max_args is not None and len(args) > self.max_args:\n",
    "            raise ArgumentException()\n",
    "        if self.max_dims and any(np.ndim(arg) > max_dim for arg, max_dim in zip(args, self.max_dims)):\n",
    "            raise ArgumentException()\n",
    "        arg_values = list(map(lambda jlv: jlv.value, args))\n",
    "        output_value, adjoint_fns = self.function(*arg_values)\n",
    "        result = JacobLspValue(\n",
    "            output_value,\n",
    "            adjoint_fns,\n",
    "            args if adjoint_fns else [],\n",
    "        )\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_arr_token= re.compile(\"^-?(\\d+,)*-?\\d+,?$\")\n",
    "float_arr_token= re.compile(\"^(-?\\d+(\\.\\d*)?,)*-?\\d+(\\.\\d*)?,?$\")\n",
    "\n",
    "def jacoblsp_atomizer(token):\n",
    "    if int_arr_token.match(token):\n",
    "        return JacobLspValue(np.array(list(map(int, filter(bool, token.split(',')))), dtype='int'))\n",
    "    if int_arr_token.match(token):\n",
    "        return JacobLspValue(np.array(list(map(float, filter(bool, token.split(',')))), dtype='float'))\n",
    "    raise ValueError(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "jplus = JacobLspOperator(\n",
    "    '+',\n",
    "    lambda *args: (\n",
    "        np.stack(args).sum(axis=0),\n",
    "        [(lambda g: g.dot(a)) for a in args]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "jminus = JacobLspOperator(\n",
    "    '-',\n",
    "    lambda *args: (\n",
    "        args[0] - np.stack(args[1:]).sum(axis=0),\n",
    "        [lambda g: g.dot(args[0])] + [(lambda g: g.dot(np.full_like(a, -1))) for a in args[1:]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "jprod = JacobLspOperator(\n",
    "    '*',\n",
    "    lambda *args: (\n",
    "        np.stack(args).prod(axis=0),\n",
    "        [(lambda g: g.dot(np.stack(args[:i] + args[i+1:]))) for i in range(len(args))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "matvecdot = (lambda A, x: [\n",
    "    lambda g: np.tensordot(g, x, 0),\n",
    "    lambda g: np.tensordot(g, A, [-1, 0])\n",
    "])\n",
    "\n",
    "vecmatdot = (lambda a, B: [\n",
    "    lambda g: np.tensordot(g, B.T, 1),\n",
    "    lambda g: np.tensordot(g, a, 0)\n",
    "])\n",
    "\n",
    "vecvecdot = (lambda a, b: [\n",
    "    lambda g: b,\n",
    "    lambda g: a,\n",
    "])\n",
    "\n",
    "matmatdot = (lambda A, B: [\n",
    "    lambda g: np.tensordot(g, B.T, 1),\n",
    "    lambda g: np.tensordot(g, A, [-2, 0]).T\n",
    "])\n",
    "\n",
    "selectdot = ({\n",
    "    (1,1): vecvecdot,\n",
    "    (2,1): matvecdot,\n",
    "    (1,2): vecmatdot,\n",
    "    (2,2): matmatdot,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdot = JacobLspOperator(\n",
    "    '*.',\n",
    "    lambda A, B: (\n",
    "        np.dot(A, B),\n",
    "        selectdot[A.ndim, B.ndim](A,B)\n",
    "    ),\n",
    "    max_args = 2,\n",
    "    max_dims = [2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "jquot = JacobLspOperator(\n",
    "    '/',\n",
    "    lambda x, y: (\n",
    "        x / y,\n",
    "        [lambda g: g/y, lambda g: g.dot(-(x/(math.pow(y, 2))))],\n",
    "    ),\n",
    "    max_args = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcos = JacobLspOperator(\n",
    "    'cos',\n",
    "    lambda x: (\n",
    "        np.cos(x),\n",
    "        [lambda g: g.dot(-np.sin(x))]\n",
    "    ),\n",
    "    max_args = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsin = JacobLspOperator(\n",
    "    'sin',\n",
    "    lambda x: (\n",
    "        np.sin(x),\n",
    "        [lambda g: g.dot(np.cos(x))],\n",
    "    ),\n",
    "    max_args = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfill = JacobLspOperator(\n",
    "    'full',\n",
    "    lambda fill, *shp: (\n",
    "        np.full(shp[0] if len(shp) == 1 else shp, fill),\n",
    "        None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = JacobLspOperator(\n",
    "    'relu',\n",
    "    lambda x: (\n",
    "        np.where(x < 0, 0, x),\n",
    "        [lambda g: (g * np.where(x < 0, 0, 1)),],\n",
    "    ),\n",
    "    max_args=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = JacobLspOperator(\n",
    "    'stack',\n",
    "    lambda *args: (np.stack(args), [lambda g: g[i] for i in range(len(args))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = JacobLspOperator(\n",
    "    'T',\n",
    "    lambda arg: (arg.T, [lambda g: g.T]) if len(arg.shape) == 2 else (arg.unsqueeze(0), [lambda g: g.unsqueeze(0)]),\n",
    "    max_args = 1,\n",
    "    max_dims = [2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacoblsp = FBHookLsp(\n",
    "    JacobLspValue,\n",
    "    JacobLspOperator,\n",
    "    jacoblsp_atomizer,\n",
    "    { f.symbol: f for f in [jplus, jminus, jprod, jquot, jsin, jcos, jdot, jfill, relu, stack, transpose] }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[0, 1, 1, 0]]),\n",
       "  [(array([[-2,  1]]), [(array([-2,  1]),)]), (array([[1, 0, 0, 0],\n",
       "           [2, 1, 1, 0]]), [(array([[ 1,  0,  0, -1],\n",
       "             [ 2,  1,  1,  0]]), [(array([[ 1,  1, -1],\n",
       "               [ 1,  1,  0]]), [(array([ 1,  1, -1]),), (array([1, 1, 0]),)]),\n",
       "       (array([[1, 0, 1, 0],\n",
       "               [1, 1, 0, 0],\n",
       "               [1, 1, 1, 1]]), [(array([[1, 1, 1],\n",
       "                 [0, 1, 1],\n",
       "                 [1, 0, 1],\n",
       "                 [0, 0, 1]]),\n",
       "          [(array([1, 1, 1]),),\n",
       "           (array([0, 1, 1]),),\n",
       "           (array([1, 0, 1]),),\n",
       "           (array([0, 0, 1]),)])])])])]),\n",
       " (array([[1., 1., 1., 1.]]),\n",
       "  [(array([[1., 4.]]), [(array([1., 4.]),)]), (array([[-2., -2., -2., -2.],\n",
       "           [ 1.,  1.,  1.,  1.]]), [(array([[-2., -2., -2., -0.],\n",
       "             [ 1.,  1.,  1.,  1.]]), [(array([[-4., -4., -6.],\n",
       "               [ 2.,  2.,  4.]]),\n",
       "        [(array([2., 2., 4.]),), (array([2., 2., 4.]),)]),\n",
       "       (array([[-1., -1., -1.,  1.],\n",
       "               [-1., -1., -1.,  1.],\n",
       "               [ 2.,  2.,  2.,  0.]]), [(array([[-1., -1.,  2.],\n",
       "                 [-1., -1.,  2.],\n",
       "                 [-1., -1.,  2.],\n",
       "                 [ 1.,  1.,  0.]]),\n",
       "          [(array([1., 1., 0.]),),\n",
       "           (array([1., 1., 0.]),),\n",
       "           (array([1., 1., 0.]),),\n",
       "           (array([1., 1., 0.]),)])])])])]))"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacoblsp.eval_str('(*. (stack -2,1) (relu (*. (stack 1,1,-1 1,1,0) (T (stack 1,1,1 0,1,1 1,0,1 0,0,1)))))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
